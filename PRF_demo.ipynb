{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Receptive Field mapping\n",
    "\n",
    "It's important to understand the responses of single units (in the case of fMRI, these units are voxels). Do we understand the computations being performed in the voxel? Well, we can actually know this. We understand what a voxel is doing when we can predict its responses to unseen stimuli. We do this by creating models that generate the responses we measure, and see whether these models can explain the data and predict unseen data. Furthermore, we can set up multiple models to explain our data, and compare their performance. \n",
    "\n",
    "In the examples shown in this brief notebook, you will investigate whether we can discriminate a response from a model voxel with an inhibition-field from a model voxel without an inhibition-field. Along the way, you will learn about population receptive fields and fitting. You'll also become acquainted with the concept of signal to noise ratio. We're not going to be doing anything with actual data in this workshop. You may think, \"what a shame!\", but when learning about these things, it's usually most instructive to be in full control of the situation, and that means generating the data yourself. Once you understand things using simulated data, you graduate to using real data because you've honed your interpretative instincts.\n",
    "\n",
    "This workshop demo is implemented in a JuPyteR Notebook. Notebooks are composed of two types of cells: MarkDown cells, like the one you're reading from now, have text in them. Code cells on the other hand, declare code fragments. When you execute a code cell, the code in it is executed/run by an interpreter, or 'kernel' in the background. In this case we're using a Python kernel, but it could also be Julia, R (hence the name) or even MATLAB. The code cells can generate images and movies, so that we have a single place to engage with text, code, and research output. The nice thing about this notebook format is that you can change the code and thereby investigate how this changes the underlying mechanisms, because changes in the code will be reflected in changes to the figures. This workshop notebook is meant for you to run through at your own pace, trying stuff out as you go along - I know, I'm a lazy teacher. To get to know the interface a bit, just spend a couple of minutes on trying out the interface and checking out the menus. \n",
    "\n",
    "Lastly, if your interest is piqued by this notebook and you want to learn/do more - specifically regarding advanced data analysis in python, I seriously recommend the [data science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/), which is built up out of notebooks just like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a comment, it will not be executed\n",
    "# everything in this cell is important, \n",
    "# because it imports functionality that we'll use in the rest of the notebook\n",
    "\n",
    "# These are general and plotting imports\n",
    "import ctypes, multiprocessing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.html'] = 'html5'\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "# These are pRF-specific imports\n",
    "import popeye.og_hrf as og # this is a receptive field model without inhibition\n",
    "import popeye.dog as dog # this is a receptive field model WITH inhibition\n",
    "import popeye.utilities as utils\n",
    "from popeye.visual_stimulus import VisualStimulus, simulate_bar_stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population receptive field model definition\n",
    "\n",
    "The nice thing about trying to understand the BOLD time-course with our model-based analysis, is that we have a fully parametrized model. This means that we can vary the parameters to create specific model instances - and, very importantly, that we can fit these parameters to our data. \n",
    "Today, you will be looking at both. But first, let's have a first look at how a receptive field model works, and what the different parameters are, starting with a simple 1D-example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function defines a one-dimensional gaussian receptive field\n",
    "def gauss_rf_1d(x, x0=0, s=1):\n",
    "    return np.exp(-((x-x0)**2)/(2*s**2))\n",
    "\n",
    "x = np.linspace(-7,7,100)\n",
    "f = plt.figure(figsize=(16,5))\n",
    "f.suptitle('Simple 1D Gaussian Models, with and without inhibitory surround',fontsize=14)\n",
    "# define and plot a standard, gaussian receptive field in 1D\n",
    "s1 = f.add_subplot(121)\n",
    "plt.axvline(0, c='k', ls='--', lw=0.5)\n",
    "plt.axhline(0, c='k', ls='--', lw=0.5)\n",
    "\n",
    "plt.plot(x, gauss_rf_1d(x, x0=3.0, s=1.4), 'r')\n",
    "plt.xlabel('Horizontal space',fontsize=12)\n",
    "plt.ylabel('Amplitude',fontsize=12);\n",
    "# let's draw some arrows\n",
    "plt.annotate(\"Amplitude\", xy=(3.0, 1.0), xytext=(3, 0.025), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='center')\n",
    "plt.annotate(\"Width ($\\sigma$)\", xy=(1.4, 0.5), xytext=(4.75, 0.5), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='left', verticalalignment='center')\n",
    "plt.annotate(\"Position (X)\", xy=(3.0, 1), xytext=(0,1), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='right', verticalalignment='center')\n",
    "\n",
    "# and, let's now implement an inhibitory surround\n",
    "s2 = f.add_subplot(122, sharey=s1)\n",
    "plt.axvline(0, c='k', ls='--', lw=0.5)\n",
    "plt.axhline(0, c='k', ls='--', lw=0.5)\n",
    "\n",
    "plt.plot(x, 1.5*gauss_rf_1d(x, x0=3.0, s=1.4)-0.5*gauss_rf_1d(x, x0=3.0, s=2.4), 'b')\n",
    "plt.xlabel('Horizontal space',fontsize=12)\n",
    "# let's draw some arrows\n",
    "plt.annotate(\"Amplitude\", xy=(3.0, 1.0), xytext=(3, 0.025), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='center')\n",
    "plt.annotate(\"Width ($\\sigma$)\", xy=(1.6, 0.5), xytext=(4.5, 0.5), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='left', verticalalignment='center')\n",
    "plt.annotate(\"Position (X)\", xy=(3.0, 1), xytext=(0,1), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='right', verticalalignment='center')\n",
    "plt.annotate(\"Inhibition\\nAmplitude\", xy=(-0.7, -0.1), xytext=(-0.7,.07), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"<->\"), horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(\"Inhibition Width ($\\sigma$)\", xy=(-0.4, -0.15), xytext=(4.75, -0.15), xycoords=(\"data\", \"data\"), \n",
    "        arrowprops=dict(arrowstyle=\"->\"), horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "sns.despine(fig=f, offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Have a look at the figure above. \n",
    "- Which is the more complex model, and why? \n",
    "- What are the differences between the two receptive fields? \n",
    "- And, what are the *2* parameters that define this difference? \n",
    "- Important: How are these receptive fields similar?\n",
    "\n",
    "**Advanced question:** Can you find where in the code this difference was implemented? What were the values of the two parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension to 2D\n",
    "\n",
    "Let's extend this concept to 2Dimensional space, because visual space is 2-dimensional of course. \n",
    "Below, we're defining a couple of functions to work with 2-dimensional receptive fields. (Not too important for you to read :-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_rf_2d(x, y, x0=0, y0=0, s=1, theta=0, ar=1):\n",
    "    \"\"\"gauss_rf_2d takes 2D arrays x and y and produces a pRF with given parameters\"\"\"\n",
    "    xr = (x-x0) * np.cos(theta) + (y-y0) * np.sin(theta)\n",
    "    yr = -(x-x0) * np.sin(theta) + (y-y0) * np.cos(theta)\n",
    "    return np.exp(-(xr**2 + ar**2 * yr**2)/(2*s**2))\n",
    "\n",
    "def plot_RF(X, Y, Z, ax, vmin=-0.25, n_levels=10, fs=14, title='', inits=(48, 74)):\n",
    "    \"\"\"plot_RF plots a pRF Z for X and Y horizontal and vertical space.\n",
    "    It takes the axis in which to plot as a necessary argument ax.\"\"\"\n",
    "    ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                    vmax=np.max(Z), vmin=vmin, cmap=cm.viridis)\n",
    "\n",
    "    # Adjust the limits, ticks and view angle\n",
    "    ax.set_zlim(-0.1,0.6)\n",
    "    ax.view_init(inits[0], inits[1])\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "\n",
    "# Our 2-dimensional distribution will be over variables X and Y\n",
    "N = 75\n",
    "X = np.linspace(-20, 20, N)\n",
    "Y = np.linspace(-20, 20, N)\n",
    "X, Y = np.meshgrid(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "In the code cells below you will see that models, and parameters for our models are declared. Read through the code and try to understand what they mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS\n",
    "## define the parameters for our pRF(s) \n",
    "x = -2.24              # Horizontal position of pRF center\n",
    "y = 2.58               # Vertical position of pRF center\n",
    "sigma = 3.74           # Width of pRF center\n",
    "sigma_ratio = 1.5      # Ratio of pRF surround width to pRF center width\n",
    "volume_ratio = 1.2     # Ratio of pRF surround amplitude to pRF center amplitude (encoded as volume here)\n",
    "beta = 0.55            # pRF height (Amplitude, also called beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize models\n",
    "In the code cells we plot the two different models, with and without inhibition. Try to use the `inits` argument to the plotting function to rotate around the plotted receptive fields. \n",
    "\n",
    "Also, you can now see how changing the parameters influences what they look like!\n",
    "\n",
    "**Advanced Question:** As you can see, there are also other options that we can change in the `gauss_rf_2d` function. These are the orientation (`theta`) and aspect ratio (`ar`) of the gauss. Feel free to play around with those in the visualizations below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# Create a standard gaussian receptive field\n",
    "Z = gauss_rf_2d(X, Y, x0=x, y0=y, s=sigma, theta=0, ar=1)\n",
    "\n",
    "# Create a surface plot and projected filled contour plot under it.\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "plot_RF(X, Y, Z, ax1, title='Gauss')\n",
    "\n",
    "# Gaussian receptive field with inhibitory surround:\n",
    "# a Difference of Gaussians model\n",
    "Z_pos = (1+volume_ratio) * gauss_rf_2d(X, Y, x0=x, y0=y, s=sigma, theta=0, ar=1)\n",
    "Z_neg = -volume_ratio * gauss_rf_2d(X, Y, x0=x, y0=y, s=sigma*sigma_ratio, theta=0, ar=1)\n",
    "Z = Z_pos + Z_neg\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "plot_RF(X, Y, Z, ax2, title='Difference of Gaussians pRF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating an actual (model) experiment!\n",
    "\n",
    "### First: Stimulus design\n",
    "\n",
    "Population receptive field mapping is usually performed with 'sweeping bar' stimuli. This means that a bar-shaped stimulus is swept across the screen in different directions. Of course, this is an fMRI experiment, so that everything happens very slowly. \n",
    "\n",
    "Let's first create a stimulus design, in which bar stimulus sweeps are alternated with blanks. The result of creating a stimulus like this, is that we have a 3D experimental design matrix. The dimensions stand for horizontal and vertical space, and time. We'll plot the resulting experimental design matrix as a movie, so that you'll get an intuition for what our model observer (and his/her receptive fields) will see. You can, of course, also use actual movies, but then the resulting experimental design becomes quite a bit more complicated.\n",
    "\n",
    "Note that we'll also be using this bar-traversal experimental design in our analysis, later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stimulus for creating time-courses\n",
    "### STIMULUS\n",
    "sweeps = np.array([-1,45,135,-1,225,315,-1]) # in degrees, -1 is blank\n",
    "# we could make it longer by adding these to sweeps: 0,90,-1,180,270,-1\n",
    "# but that will make fitting slower, later. \n",
    "## create sweeping bar stimulus\n",
    "bar = simulate_bar_stimulus(100, 100, 40, 40, sweeps, 30, 30, 20, 0.67)\n",
    "stimulus = VisualStimulus(bar, 50, 25, 0.50, 1.0, ctypes.c_int16)\n",
    "\n",
    "# plot the stimulus as it traverses the screen\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ims = []\n",
    "for t in range(stimulus.stim_arr.shape[-1]):\n",
    "    im = plt.imshow(stimulus.stim_arr[...,t], animated=True, cmap='viridis', interpolation='bicubic');\n",
    "    plt.axis('off')\n",
    "    ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig, ims, interval=80, blit=True, repeat_delay=1500)\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual analysis\n",
    "\n",
    "Now, we can use our model experiment to stimulate our model receptive fields, in a simulation of an experimental recording. We can run the same experiment for different model receptive fields, and then investigate their differences. Do the different models produce different BOLD time-courses?\n",
    "\n",
    "\n",
    "Later, we will add noise of a specific level, so that we can investigate whether we can still find differences between two models.\n",
    "\n",
    "For this demo, we'll be using the nice python pRF fitting package popeye, because implementing our own here would be a bit too much for a two-hour workshop. \n",
    "\n",
    "First, we'll have a look at how these receptive fields activate when we play our movie to them. What we do, is that we multiply our stimulus design matrix with the pRF for each timepoint. This gives us a time series of single values each representing the overlap between stimulus and pRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS - REPEATED HERE\n",
    "## define the parameters for our pRF(s) \n",
    "x = -2.24              # Horizontal position of pRF center\n",
    "y = 2.58               # Vertical position of pRF center\n",
    "sigma = 3.74           # Width of pRF center\n",
    "sigma_ratio = 1.5      # Ratio of pRF surround width to pRF center width\n",
    "volume_ratio = 1.2     # Ratio of pRF surround amplitude to pRF center amplitude (encoded as volume here)\n",
    "beta = 0.55            # pRF height (Amplitude, also called beta)\n",
    "\n",
    "### THESE PARAMETERS ARE IMPORTANT FOR BOLD-LEVEL SIMULATIONS\n",
    "\n",
    "hrf_delay = 0          # fMRI: the HRF can vary between voxels: we'll leave it at 0\n",
    "baseline = -0.28       # baseline, was 0 in plots above, but \n",
    "                       # for fMRI recordings (and later in this notebook) this is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the gaussian and difference of gaussian models\n",
    "## and create the time-series for the invented pRF estimate\n",
    "og_model = og.GaussianModel(stimulus, utils.spm_hrf)\n",
    "og_data = og_model.generate_prediction(x, y, sigma, hrf_delay, beta, baseline)\n",
    "\n",
    "dog_model = dog.DifferenceOfGaussiansModel(stimulus, utils.spm_hrf)\n",
    "dog_model.hrf_delay = hrf_delay\n",
    "dog_data = dog_model.generate_prediction(x, y, sigma, sigma_ratio, volume_ratio, beta, baseline)/200\n",
    "\n",
    "## plot the noise-free simulations\n",
    "f = plt.figure(figsize=(16,8))\n",
    "f.suptitle('The precise timings and shapes of these activations and deactivations result from our parameters.')\n",
    "\n",
    "s1 = f.add_subplot(211)\n",
    "s1.axhline(0, c='k', ls='--', lw=0.5)\n",
    "s1.plot(og_data,c='b',lw=3,label='Gaussian model',zorder=1)\n",
    "s1.set_ylabel('Amplitude',fontsize=14)\n",
    "s1.set_xlim(0,stimulus.stim_arr.shape[-1])\n",
    "s1.legend(loc=1)\n",
    "s2 = f.add_subplot(212)\n",
    "s2.axhline(0, c='k', ls='--', lw=0.5)\n",
    "s2.plot(dog_data,c='r',lw=3,label='Difference of Gaussian model',zorder=1)\n",
    "# note the weird scaling of the dog time-course. \n",
    "# this is something that we inherit from popeye, the fitting package\n",
    "s2.set_xlabel('Time',fontsize=14)\n",
    "s2.set_ylabel('Amplitude',fontsize=14)\n",
    "s2.set_xlim(0,stimulus.stim_arr.shape[-1])\n",
    "s2.legend(loc=1)\n",
    "\n",
    "sns.despine(fig=f, offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "Try to get an intuition regarding how the above time-courses are dependent on specific stimulus parameters by changing some of the parameters, like the `x` and `y` position of the pRFs. Do you see shifts, amplifications, etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the pRFs\n",
    "\n",
    "In the code cell below we're implementing a few functions that 1. add noise to our simulations, and 2. fit the noisy signals with a specific model. You can read through the code, but it's not too important. \n",
    "\n",
    "Note that fitting models like this is hard work for a computer. In the following analyses, you might have to wait a while before you can interpret the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_sim(data, noise_level):\n",
    "    noisy_data = data + np.random.randn(len(data))*data.std()*noise_level\n",
    "    return noisy_data\n",
    "\n",
    "def fit_one_sim(data, model, stimulus, verbose=1):\n",
    "    \"\"\"fit_one_sim takes a data time-course, a model instance \n",
    "    and a stimulus definition, and fits the model parameters on the time-course.\"\"\"\n",
    "    ## define search grids\n",
    "    # these define min and max of the edge of the initial brute-force search. \n",
    "    x_grid = (-10,10)\n",
    "    y_grid = (-10,10)\n",
    "    s_grid = (1/stimulus.ppd + 0.25,5.25)\n",
    "    sr_grid = (1.5,4.5)\n",
    "    vr_grid = (0.0,1.5)\n",
    "    h_grid = (-1.0,1.0)\n",
    "\n",
    "    ## define search bounds\n",
    "    # these define the boundaries of the final gradient-descent search.\n",
    "    x_bound = (-18.0,18.0)\n",
    "    y_bound = (-18.0,18.0)\n",
    "    s_bound = (1/stimulus.ppd, 12.0) # smallest sigma is a pixel\n",
    "    sr_bound = (1.1, 6.0) \n",
    "    vr_bound = (0,3.0)\n",
    "    b_bound = (1e-8,None)\n",
    "    u_bound = (None,None)\n",
    "    h_bound = (-3.0,3.0)\n",
    "        \n",
    "    ## fit the response\n",
    "    print('Fitting ' + type(model).__name__)\n",
    "\n",
    "    if type(model).__name__ == 'GaussianModel':\n",
    "        ## package the grids and bounds\n",
    "        grids = (x_grid, y_grid, s_grid, h_grid)\n",
    "        bounds = (x_bound, y_bound, s_bound, h_bound, b_bound, u_bound,)\n",
    "        # fit\n",
    "        fit = og.GaussianFit(model, data, grids, bounds, Ns=5,\n",
    "                         voxel_index=(1,2,3), auto_fit=True, verbose=1)\n",
    "    elif type(model).__name__ == 'DifferenceOfGaussiansModel':\n",
    "        ## package the grids and bounds\n",
    "        grids = (x_grid, y_grid, s_grid, sr_grid, vr_grid)\n",
    "        bounds = (x_bound, y_bound, s_bound, sr_bound, vr_bound, b_bound, u_bound,)\n",
    "        # fit\n",
    "        fit = dog.DifferenceOfGaussiansFit(model, data, grids, bounds, Ns=5,\n",
    "                         voxel_index=(1,2,3), auto_fit=True, verbose=verbose)        \n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying noise levels and what to fit\n",
    "In the first cell below, you can specify \n",
    "1. the noise level to use\n",
    "2. which dataset to fit (OG or DoG)\n",
    "3. which model to use for the fit (OG or DoG)\n",
    "\n",
    "The idea is that you try to see what each of the models can do that the other can't. Try, for example, to fit the DoG data with the OG data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noise level for simulations. Defined as fraction of data std, higher is more noisy, lower is less noisy\n",
    "noise_level = 1.0     \n",
    "# here, we can select whether to use OG or DoG data\n",
    "data2use = 'OG'\n",
    "fit2use = 'DoG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulation and fit\n",
    "In the cell below, we pretend to perform an experiment by creating noisy \"data\". This is our original modeled data - see above - with noise of amplitude `noise_level` added. Then, we will actually fit the pRF parameters using the generated noisy data. \n",
    "\n",
    "We also create a separate dataset, with different noise. You can think of this as a separate measurement from the same voxel. That is, its underlying signal is the same, but the measurement noise is different. With this extra data, we can calculate the cross-validated R$^{2}$ - this measure is robust to overfitting, whereas the R$^{2}$ of the fit data *is*.\n",
    "This is of importance because, as you saw above, the DoG model has two extra parameters: It would be more prone to overfitting than the OG model which has fewer parameters. For more information on overfitting, and how to avoid it, see the [scikit-learn website](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in data dependent on the settings above\n",
    "if data2use == 'OG':\n",
    "    present_data = og_data\n",
    "elif data2use == 'DoG':\n",
    "    present_data = dog_data\n",
    "\n",
    "if fit2use == 'OG':\n",
    "    present_model = og_model\n",
    "elif fit2use == 'DoG':\n",
    "    present_model = dog_model\n",
    "\n",
    "# we create noisy data on top of our simulated time-courses\n",
    "# once to fit on, and once to test on: cross-validated estimation is important!\n",
    "noisy_data = create_one_sim(data=present_data, noise_level=noise_level)\n",
    "noisy_test_data = create_one_sim(data=present_data, noise_level=noise_level)\n",
    "\n",
    "# the actual fit. \n",
    "# here, we can select whether to fit using an OG or DoG model\n",
    "fit = fit_one_sim(noisy_data, \n",
    "                  present_model, stimulus, verbose=0)\n",
    "\n",
    "# and quantify the CV R^2\n",
    "cv_rsq = 1.0 - np.sum((fit.prediction - noisy_test_data)\n",
    "                              ** 2) / np.sum(noisy_test_data**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the results\n",
    "f = plt.figure(figsize=(16,5))\n",
    "f.suptitle('Data from {data} fitted with {model}'.format(data=data2use, model=type(present_model).__name__))\n",
    "plt.scatter(range(len(fit.data)), fit.data, c='k', label='fit data', alpha=0.8, marker='x')\n",
    "plt.scatter(range(len(fit.data)), noisy_test_data, c='k', label='left-out (CV) data', alpha=0.3, marker='+')\n",
    "\n",
    "# fits and model timecourses\n",
    "if fit2use == 'DoG':\n",
    "    fit_color = 'b'\n",
    "elif fit2use == 'OG':\n",
    "    fit_color = 'r'\n",
    "    \n",
    "plt.plot(fit.prediction, c=fit_color,lw=3,label='fit model prediction,\\nR^2=%1.2f, CV R^2=%1.2f'%(fit.rsquared, cv_rsq))\n",
    "\n",
    "if data2use == 'DoG':\n",
    "    plt.plot(dog_data,c='b',ls=':',lw=3,label='actual DoG model')\n",
    "elif data2use == 'OG':\n",
    "    plt.plot(og_data,c='r',ls=':',lw=3,label='actual OG model')\n",
    "\n",
    "plt.xlabel('Time',fontsize=14)\n",
    "plt.ylabel('Amplitude',fontsize=14)\n",
    "plt.xlim(0,len(fit.data))\n",
    "plt.legend(loc=0)\n",
    "sns.despine(fig=f, offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Try to play around with this procedure. \n",
    "\n",
    "For example, what's the difference between:\n",
    "1. Fitting an DoG model on OG data, and \n",
    "2. Fitting an OG model on DoG data\n",
    "\n",
    "**Question:** How does the OG model try to fit the inhibition in the DoG data? What does this say about how easy it is to find inhibition using fMRI - in pRF mapping, but also in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also play with the noise levels. For the interested students, feel free to continue with our quantification of these trade-offs. For example, how do the rsq and cv_rsq values behave for the OG data and the DoG model, when you vary the noise level? We try to answer this question below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_data = og_data\n",
    "noise_levels = np.linspace(1.0,3.5,10)\n",
    "\n",
    "rsqs, cv_rsqs = [], []\n",
    "for nl in noise_levels:\n",
    "    # we create noisy data on top of our simulated time-courses\n",
    "    # once to fit on, and once to test on: cross-validated estimation is important!\n",
    "    noisy_data = create_one_sim(data=present_data, noise_level=nl)\n",
    "    noisy_test_data = create_one_sim(data=present_data, noise_level=nl)\n",
    "\n",
    "    # the actual fit. \n",
    "    # here, we can select whether to fit using an OG or DoG model\n",
    "    fit_OG = fit_one_sim(noisy_data, og_model, stimulus, verbose=1)\n",
    "    fit_DoG = fit_one_sim(noisy_data, dog_model, stimulus, verbose=1)\n",
    "    \n",
    "    # and quantify the CV R^2\n",
    "    cv_rsq_OG = 1.0 - np.sum((fit_OG.prediction - noisy_test_data)\n",
    "                                  ** 2) / np.sum(noisy_test_data**2)\n",
    "    cv_rsq_DoG = 1.0 - np.sum((fit_DoG.prediction - noisy_test_data)\n",
    "                                  ** 2) / np.sum(noisy_test_data**2)\n",
    "\n",
    "    \n",
    "    cv_rsqs.append([cv_rsq_OG,cv_rsq_DoG])\n",
    "    rsqs.append([fit_OG.rsquared, fit_DoG.rsquared])\n",
    "\n",
    "rsqs = np.array(rsqs)\n",
    "cv_rsqs = np.array(cv_rsqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,6))\n",
    "s1 = f.add_subplot(121)\n",
    "s1.plot(noise_levels, rsqs[:,0], c='c', label='Within-set R$^{2}$')\n",
    "s1.plot(noise_levels, cv_rsqs[:,0], c='m', label='Cross-validated R$^{2}$')\n",
    "s1.set_title('Gaussian pRF model')\n",
    "s1.legend()\n",
    "s1.set_ylabel('R$^{2}$')\n",
    "s1.set_xlabel('noise level')\n",
    "s2 = f.add_subplot(122, sharey=s1)\n",
    "s2.plot(noise_levels, rsqs[:,1], 'c', label='Within-set R$^{2}$')\n",
    "s2.plot(noise_levels, cv_rsqs[:,1], 'm', label='Cross-validated R$^{2}$')\n",
    "s2.set_title('Difference of Gaussians pRF model')\n",
    "s2.legend()\n",
    "s2.set_xlabel('noise level')\n",
    "sns.despine(fig=f, offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Assignment\n",
    "\n",
    "There are some basic outcomes to inspect from this tiny simulation and fitting exercise. You can answer the questions:\n",
    "- Which of the two models took longer to fit? Is it a large difference? Why do you think this is?\n",
    "- Why is the within-set or out-of-set higher? Go ahead, muse about why this is the case. **Hint:** Think about overfitting!\n",
    "- Why are the lines for the two models so similar for these simulations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced fitting\n",
    "\n",
    "You have seen that the above analysis already that takes a lot of time, mostly because the analyses are performed serially. Luckily, these fitting procedures can be paralellized over multiple processors. You can find how to do this in the brief explanation of [the possibilities of popeye](https://kdesimone.github.io/popeye/). The open assignment below is for you to re-implement the above quantification of explained variance, but now using paralllel processing. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
